{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQAstXRA5Elp",
        "outputId": "0cb89e01-15b0-4185-eacf-a8981a8d2a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'small-coder' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "![ ! -f requirements.txt ] && git clone https://github.com/anudaweerasinghe/small-coder.git && cd small-coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gEAAaukh5Elr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "! pip3 install -r small-coder/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hJ3gPyH5Elr",
        "outputId": "d204fbfd-92be-4f53-e42c-2e4f16e31bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# setup human eval and mbpp dataset\n",
        "from datasets import load_dataset\n",
        "humaneval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
        "mbpp = load_dataset(\"mbpp\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mbppToFnHeaderAndDocString(mbpp_datum):\n",
        "  ref_implementation = mbpp_datum['code']\n",
        "  prompt = mbpp_datum['text']\n",
        "\n",
        "  fn_header = ref_implementation.split('\\n', 1)[0]\n",
        "\n",
        "  return f'{fn_header}\\n    \"\"\"\\n    {prompt}\\n    \"\"\"'\n"
      ],
      "metadata": {
        "id": "oj1-E7SVXw2f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getHumanEvalTestsList(humaneval_datum):\n",
        "  check_fn = humaneval_datum['test']\n",
        "\n",
        "  lines = [line.strip() for line in check_fn.split('\\n')]\n",
        "\n",
        "  tests = list(filter(lambda string: string.startswith('assert'), lines))\n",
        "\n",
        "  return tests"
      ],
      "metadata": {
        "id": "-90F4kJ1iQP1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "287CxGeH5Els",
        "outputId": "7df6645b-70dd-40e1-e167-f105aa727f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================HumanEval=================\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "['assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True', 'assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False', 'assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True', 'assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False', 'assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True', 'assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True', 'assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False']\n",
            "===================MBPP===================\n",
            "def remove_Occ(s,ch): \r\n",
            "    \"\"\"\n",
            "    Write a python function to remove first and last occurrence of a given character from the string.\n",
            "    \"\"\"\n",
            "['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"', 'assert remove_Occ(\"PHP\",\"P\") == \"H\"']\n"
          ]
        }
      ],
      "source": [
        "# View one example of humaneval and mbpp\n",
        "print('=================HumanEval=================')\n",
        "print(humaneval[0]['prompt'])\n",
        "print(getHumanEvalTestsList(humaneval[0]))\n",
        "\n",
        "print('===================MBPP===================')\n",
        "print(mbppToFnHeaderAndDocString(mbpp[0]))\n",
        "print(mbpp[0]['test_list'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhXWgv1T5Els",
        "outputId": "9576db45-1aae-421a-bfc9-93c8427b39ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#initialize deepsek-coder-1.3b\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper to generate code for agiven docstring\n",
        "def generateCode(docstring):\n",
        "  message = [\n",
        "    {'role': 'user', 'content': 'Complete the following code. Do not generate test cases.'},\n",
        "    { 'role': 'user', 'content': docstring  }\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "  # tokenizer.eos_token_id is the id of <|EOT|> token\n",
        "  outputs = model.generate(inputs, max_new_tokens=128000, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "7TLcK--pfPBE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mybevull5Els",
        "outputId": "02cfd294-6013-4bb6-e0d2-223528c454d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================HumanEval=================\n",
            "from typing import List\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for i in range(len(numbers)):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) < threshold:\n",
            "                return True\n",
            "    return False\n",
            "\n",
            "===================MBPP===================\n",
            "def remove_Occ(s,ch): \r\n",
            "    \"\"\"\n",
            "    Write a python function to remove first and last occurrence of a given character from the string.\n",
            "    \"\"\"\n",
            "    if ch in s:\r\n",
            "        s = s.replace(ch, '', 1)\r\n",
            "        s = s.replace(ch, '', 1)\r\n",
            "    return s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "humanevalExampleResult = generateCode(humaneval[0]['prompt'])\n",
        "mbppExampleResult = generateCode(mbppToFnHeaderAndDocString(mbpp[0]))\n",
        "\n",
        "print('=================HumanEval=================')\n",
        "print(humanevalExampleResult)\n",
        "\n",
        "print('===================MBPP===================')\n",
        "print(mbppExampleResult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F5CX-_EB7sQk"
      },
      "outputs": [],
      "source": [
        "# allow running code for evaluation - THIS IS DANGEROUS\n",
        "import os\n",
        "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "code_eval = load(\"code_eval\")"
      ],
      "metadata": {
        "id": "8yO_VdJLk3g7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateCode(code, tests):\n",
        "  print(code)\n",
        "  print(tests)\n",
        "  candidates = [[code]]\n",
        "\n",
        "  pass_at_k, results = code_eval.compute(references=tests, predictions=candidates, k=[1])\n",
        "\n",
        "  return pass_at_k, results"
      ],
      "metadata": {
        "id": "mMWLyu4glQ_W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [getHumanEvalTestsList(humaneval[0])[0]]\n",
        "candidates = [[humanevalExampleResult]]\n",
        "\n",
        "print(test_cases, candidates)\n",
        "pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1])\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2XW6TLXTlusY",
        "outputId": "4f986b15-2576-485d-f45c-353f8a394ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True'] [['from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False\\n']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': \"failed: name 'candidate' is not defined\", 'completion_id': 0})]})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8ORHFVOqqxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}