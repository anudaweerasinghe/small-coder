{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"anudaw/full_finetuned-code-tinyllama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"anudaw/full_finetuned-code-tinyllama\", trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_length = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def generate_results(model, tokenizer, entries, output_file, num_samples=5):\n",
    "  results = []\n",
    "\n",
    "  num_entries = len(entries)\n",
    "  batch_size = 1\n",
    "\n",
    "  for batch in tqdm(np.array_split(entries, math.ceil(num_entries / batch_size))):\n",
    "    prompts = [row['prompt'] for (_, row) in batch.iterrows()]\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "      output_ids = model.generate(**input_ids, max_length=max_length, do_sample=True, temperature=0.2, num_return_sequences=num_samples)\n",
    "    \n",
    "    cur_list = []\n",
    "    for i, output in enumerate(output_ids):\n",
    "      generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "      cur_list.append(generated_text)\n",
    "\n",
    "      if len(cur_list) == num_samples:\n",
    "        results.append({ 'prompt': prompts[i // num_samples], 'samples': cur_list })\n",
    "        json.dump({ 'prompt': prompts[i // num_samples], 'samples': cur_list }, output_file)\n",
    "        output_file.write('\\n')\n",
    "        cur_list = []\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "dataset_folder = Path('../../CodeT/CodeT/data/dataset')\n",
    "dataset_name = 'HumanEval'\n",
    "\n",
    "codegen_file = dataset_folder / f'{dataset_name}_for_code_generation.jsonl'\n",
    "testcase_file = dataset_folder / f'{dataset_name}_for_test_case_generation.jsonl'\n",
    "\n",
    "codegen_json = pd.read_json(path_or_buf=Path(codegen_file), lines=True)\n",
    "testcase_json = pd.read_json(path_or_buf=Path(testcase_file), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [58:06<00:00, 21.26s/it] \n"
     ]
    }
   ],
   "source": [
    "with open(f'{dataset_name}-testcase-20-temp0.2.jsonl', mode='w') as writer:\n",
    "    results = generate_results(model, tokenizer, testcase_json, writer, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [50:09<00:00, 18.35s/it] \n"
     ]
    }
   ],
   "source": [
    "with open(f'{dataset_name}-codegen-20-temp0.2.jsonl', mode='w') as writer:\n",
    "    results = generate_results(model, tokenizer, codegen_json, writer, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "codegen_20_json = pd.read_json(path_or_buf=Path('./HumanEval-codegen-60-temp0.5.jsonl'), lines=True)\n",
    "testcase_20_json = pd.read_json(path_or_buf=Path('./HumanEval-testcase-60-temp0.5.jsonl'), lines=True)\n",
    "\n",
    "with open(f'HumanEval-codegen-60-temp0.5-suffix.jsonl', mode='w') as writer:\n",
    "\tfor i, row in codegen_20_json.iterrows():\n",
    "\t\tprompt = row['prompt']\n",
    "\t\tsamples = row['samples']\n",
    "\t\tnew_samples = []\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tnew_samples.append(sample[len(prompt):])\n",
    "\t\tjson.dump({ 'prompt': prompt, 'samples': new_samples }, writer)\n",
    "\t\twriter.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
